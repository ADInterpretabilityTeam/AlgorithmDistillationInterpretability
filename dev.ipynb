{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Transformer Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dataclasses import dataclass, field\n",
    "import gymnasium as gym\n",
    "\n",
    "@dataclass\n",
    "class TransformerModelConfig():\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    d_mlp: int = 256\n",
    "    n_layers: int = 2\n",
    "    n_ctx: int = 3\n",
    "    layer_norm: bool = False\n",
    "    linear_time_embedding: bool = False\n",
    "    state_embedding_type: str = 'grid'\n",
    "    time_embedding_type: str = 'learned'\n",
    "    seed: int = 1\n",
    "    device: str = 'cpu'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "\n",
    "transformer_config = TransformerModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(7)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class EnvironmentConfig():\n",
    "    env = None\n",
    "    env_id: str = 'MiniGrid-Empty-8x8-v0'\n",
    "    one_hot: bool = False\n",
    "    fully_observed: bool = False\n",
    "    max_steps: int = 1000\n",
    "    seed: int = 1\n",
    "    view_size: int = 7\n",
    "    capture_video: bool = False\n",
    "    video_dir: str = 'videos'\n",
    "    render_mode: str = 'rgb_array'\n",
    "    num_parralel_envs: int = 1\n",
    "    action_space: None = None\n",
    "    observation_space: None = None\n",
    "    device: str = 'cpu'\n",
    "    \n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        env = gym.make(self.env_id)\n",
    "        self.action_space = env.action_space or env.action_space\n",
    "        self.observation_space = env.observation_space or env.observation_space\n",
    "\n",
    "\n",
    "environment_config = EnvironmentConfig()\n",
    "environment_config.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(7)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment_config.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Model 1 Shape</th>\n",
       "      <th>Model 2 Shape</th>\n",
       "      <th>Sizes Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>action_embedding.0.weight</td>\n",
       "      <td>(8, 128)</td>\n",
       "      <td>(8, 128)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reward_embedding.0.weight</td>\n",
       "      <td>(128, 1)</td>\n",
       "      <td>(128, 1)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>time_embedding.weight</td>\n",
       "      <td>(1001, 128)</td>\n",
       "      <td>(1001, 128)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>state_embedding.weight</td>\n",
       "      <td>(1001, 128)</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transformer.pos_embed.W_pos</td>\n",
       "      <td>(3, 128)</td>\n",
       "      <td>(3, 128)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>transformer.blocks.0.attn.W_Q</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>transformer.blocks.0.attn.W_K</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>transformer.blocks.0.attn.W_V</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>transformer.blocks.0.attn.W_O</td>\n",
       "      <td>(4, 32, 128)</td>\n",
       "      <td>(4, 32, 128)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>transformer.blocks.0.attn.b_Q</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>transformer.blocks.0.attn.b_K</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>transformer.blocks.0.attn.b_V</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>transformer.blocks.0.attn.b_O</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>transformer.blocks.0.attn.mask</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>transformer.blocks.0.attn.IGNORE</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>transformer.blocks.0.mlp.W_in</td>\n",
       "      <td>(128, 256)</td>\n",
       "      <td>(128, 256)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>transformer.blocks.0.mlp.b_in</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>transformer.blocks.0.mlp.W_out</td>\n",
       "      <td>(256, 128)</td>\n",
       "      <td>(256, 128)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>transformer.blocks.0.mlp.b_out</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>transformer.blocks.1.attn.W_Q</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>transformer.blocks.1.attn.W_K</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>transformer.blocks.1.attn.W_V</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>(4, 128, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>transformer.blocks.1.attn.W_O</td>\n",
       "      <td>(4, 32, 128)</td>\n",
       "      <td>(4, 32, 128)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>transformer.blocks.1.attn.b_Q</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>transformer.blocks.1.attn.b_K</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>transformer.blocks.1.attn.b_V</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>(4, 32)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>transformer.blocks.1.attn.b_O</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>transformer.blocks.1.attn.mask</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>transformer.blocks.1.attn.IGNORE</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>transformer.blocks.1.mlp.W_in</td>\n",
       "      <td>(128, 256)</td>\n",
       "      <td>(128, 256)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>transformer.blocks.1.mlp.b_in</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>(256,)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>transformer.blocks.1.mlp.W_out</td>\n",
       "      <td>(256, 128)</td>\n",
       "      <td>(256, 128)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>transformer.blocks.1.mlp.b_out</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>action_predictor.weight</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>action_predictor.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>reward_predictor.weight</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>reward_predictor.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>predict_states.weight</td>\n",
       "      <td>(147, 128)</td>\n",
       "      <td>(147, 128)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>predict_states.bias</td>\n",
       "      <td>(147,)</td>\n",
       "      <td>(147,)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>state_encoder.weight</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>(128, 147)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>predict_actions.weight</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>(7, 128)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>predict_actions.bias</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>(7,)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>predict_rewards.weight</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>(1, 128)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>predict_rewards.bias</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Parameter Model 1 Shape Model 2 Shape  Sizes Match\n",
       "0          action_embedding.0.weight      (8, 128)      (8, 128)         True\n",
       "1          reward_embedding.0.weight      (128, 1)      (128, 1)         True\n",
       "2              time_embedding.weight   (1001, 128)   (1001, 128)         True\n",
       "3             state_embedding.weight   (1001, 128)       MISSING        False\n",
       "4        transformer.pos_embed.W_pos      (3, 128)      (3, 128)         True\n",
       "5      transformer.blocks.0.attn.W_Q  (4, 128, 32)  (4, 128, 32)         True\n",
       "6      transformer.blocks.0.attn.W_K  (4, 128, 32)  (4, 128, 32)         True\n",
       "7      transformer.blocks.0.attn.W_V  (4, 128, 32)  (4, 128, 32)         True\n",
       "8      transformer.blocks.0.attn.W_O  (4, 32, 128)  (4, 32, 128)         True\n",
       "9      transformer.blocks.0.attn.b_Q       (4, 32)       (4, 32)         True\n",
       "10     transformer.blocks.0.attn.b_K       (4, 32)       (4, 32)         True\n",
       "11     transformer.blocks.0.attn.b_V       (4, 32)       (4, 32)         True\n",
       "12     transformer.blocks.0.attn.b_O        (128,)        (128,)         True\n",
       "13    transformer.blocks.0.attn.mask        (3, 3)        (3, 3)         True\n",
       "14  transformer.blocks.0.attn.IGNORE            ()            ()         True\n",
       "15     transformer.blocks.0.mlp.W_in    (128, 256)    (128, 256)         True\n",
       "16     transformer.blocks.0.mlp.b_in        (256,)        (256,)         True\n",
       "17    transformer.blocks.0.mlp.W_out    (256, 128)    (256, 128)         True\n",
       "18    transformer.blocks.0.mlp.b_out        (128,)        (128,)         True\n",
       "19     transformer.blocks.1.attn.W_Q  (4, 128, 32)  (4, 128, 32)         True\n",
       "20     transformer.blocks.1.attn.W_K  (4, 128, 32)  (4, 128, 32)         True\n",
       "21     transformer.blocks.1.attn.W_V  (4, 128, 32)  (4, 128, 32)         True\n",
       "22     transformer.blocks.1.attn.W_O  (4, 32, 128)  (4, 32, 128)         True\n",
       "23     transformer.blocks.1.attn.b_Q       (4, 32)       (4, 32)         True\n",
       "24     transformer.blocks.1.attn.b_K       (4, 32)       (4, 32)         True\n",
       "25     transformer.blocks.1.attn.b_V       (4, 32)       (4, 32)         True\n",
       "26     transformer.blocks.1.attn.b_O        (128,)        (128,)         True\n",
       "27    transformer.blocks.1.attn.mask        (3, 3)        (3, 3)         True\n",
       "28  transformer.blocks.1.attn.IGNORE            ()            ()         True\n",
       "29     transformer.blocks.1.mlp.W_in    (128, 256)    (128, 256)         True\n",
       "30     transformer.blocks.1.mlp.b_in        (256,)        (256,)         True\n",
       "31    transformer.blocks.1.mlp.W_out    (256, 128)    (256, 128)         True\n",
       "32    transformer.blocks.1.mlp.b_out        (128,)        (128,)         True\n",
       "33           action_predictor.weight        (128,)       MISSING        False\n",
       "34             action_predictor.bias        (128,)       MISSING        False\n",
       "35           reward_predictor.weight        (128,)       MISSING        False\n",
       "36             reward_predictor.bias        (128,)       MISSING        False\n",
       "37             predict_states.weight    (147, 128)    (147, 128)         True\n",
       "38               predict_states.bias        (147,)        (147,)         True\n",
       "39              state_encoder.weight       MISSING    (128, 147)        False\n",
       "40            predict_actions.weight       MISSING      (7, 128)        False\n",
       "41              predict_actions.bias       MISSING          (7,)        False\n",
       "42            predict_rewards.weight       MISSING      (1, 128)        False\n",
       "43              predict_rewards.bias       MISSING          (1,)        False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from typing import Dict, Union, Tuple\n",
    "from torchtyping import TensorType as TT\n",
    "from abc import ABC, abstractmethod\n",
    "from einops import rearrange\n",
    "from src.decision_transformer.model import StateEncoder, PosEmbedTokens\n",
    "from src.decision_transformer.model import DecisionTransformer as DecisionTransformerOld\n",
    "import numpy as np\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from gymnasium.spaces import Box, Dict\n",
    "# from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
    "\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "    '''\n",
    "    Base Class for trajectory modelling transformers including:\n",
    "        - Decision Transformer (offline, RTG, (R,s,a))\n",
    "        - Online Transformer (online, reward, (s,a,r))\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer_config: TransformerModelConfig,\n",
    "        environment_config: EnvironmentConfig\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer_config = transformer_config\n",
    "        self.environment_config = environment_config\n",
    "\n",
    "\n",
    "        self.action_embedding = nn.Sequential(\n",
    "            nn.Embedding(environment_config.action_space.n+1, self.transformer_config.d_model))\n",
    "        self.reward_embedding = nn.Sequential(\n",
    "            nn.Linear(1, self.transformer_config.d_model, bias=False))\n",
    "        self.time_embedding = self.initialize_time_embedding()\n",
    "        self.state_embedding = self.initialize_state_embedding()\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.normal_(\n",
    "            self.action_embedding[0].weight, mean=0.0, std=1/((environment_config.action_space.n+1 + 1)*self.transformer_config.d_model))\n",
    "        nn.init.normal_(\n",
    "            self.reward_embedding[0].weight, mean=0.0, std=1/self.transformer_config.d_model)\n",
    "        \n",
    "        self.transformer = self.initialize_easy_transformer()\n",
    "\n",
    "        self.action_predictor = nn.Linear(self.transformer_config.d_model, environment_config.action_space.n)\n",
    "        self.reward_predictor = nn.Linear(self.transformer_config.d_model, 1)\n",
    "        self.initialize_state_predictor()\n",
    "        \n",
    "    def forward(self,\n",
    "                # has variable shape, starting with batch, position\n",
    "                states: TT[...],\n",
    "                actions: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "                rtgs: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "                timesteps: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "                ) -> Tuple[TT[...], TT[\"batch\", \"position\"], TT[\"batch\", \"position\"]]:  # noqa: F821\n",
    "\n",
    "        batch_size = states.shape[0]\n",
    "        seq_length = states.shape[1]\n",
    "\n",
    "        # embed states and recast back to (batch, block_size, n_embd)\n",
    "        token_embeddings = self.to_tokens(states, actions, rtgs, timesteps)\n",
    "        x = self.transformer(token_embeddings)\n",
    "        state_preds, action_preds, reward_preds = self.get_logits(\n",
    "            x, batch_size, seq_length)\n",
    "\n",
    "        return state_preds, action_preds, reward_preds\n",
    "\n",
    "    def get_logits(self, x, batch_size, seq_length):\n",
    "\n",
    "        # TODO replace with einsum\n",
    "        x = x.reshape(batch_size, seq_length, 3,\n",
    "                      self.transformer_config.d_model).permute(0, 2, 1, 3)\n",
    "\n",
    "        # predict next return given state and action\n",
    "        reward_preds = self.predict_rewards(x[:, 2])\n",
    "        # predict next state given state and action\n",
    "        state_preds = self.predict_states(x[:, 2])\n",
    "        # predict next action given state\n",
    "        action_preds = self.predict_actions(x[:, 1])\n",
    "\n",
    "        return state_preds, action_preds, reward_preds\n",
    "\n",
    "    def predict_rewards(self, x):\n",
    "        return self.reward_predictor(x)  \n",
    "\n",
    "    def predict_states(self, x):\n",
    "        return self.state_predictor(x)\n",
    "\n",
    "    def predict_actions(self, x):\n",
    "        return self.action_predictor(x)\n",
    "\n",
    "    def to_tokens(self, states, actions, rtgs, timesteps):\n",
    "\n",
    "        # embed states and recast back to (batch, block_size, n_embd)\n",
    "        state_embeddings = self.get_state_embeddings(\n",
    "            states)  # batch_size, block_size, n_embd\n",
    "        action_embeddings = self.get_action_embeddings(\n",
    "            actions) if actions is not None else None  # batch_size, block_size, n_embd or None\n",
    "        reward_embeddings = self.get_reward_embeddings(\n",
    "            rtgs)  # batch_size, block_size, n_embd\n",
    "        time_embeddings = self.get_time_embeddings(\n",
    "            timesteps)  # batch_size, block_size, n_embd\n",
    "\n",
    "        # use state_embeddings, actions, rewards to go and\n",
    "        token_embeddings = self.get_token_embeddings(\n",
    "            state_embeddings=state_embeddings,\n",
    "            action_embeddings=action_embeddings,\n",
    "            reward_embeddings=reward_embeddings,\n",
    "            time_embeddings=time_embeddings\n",
    "        )\n",
    "        return token_embeddings\n",
    "    \n",
    "    def get_time_embedding(self, timesteps):\n",
    "\n",
    "        assert timesteps.max() <= self.max_timestep, \"timesteps must be less than max_timesteps\"\n",
    "\n",
    "        block_size = timesteps.shape[1]\n",
    "        timesteps = rearrange(\n",
    "            timesteps, 'batch block time-> (batch block) time')\n",
    "        time_embeddings = self.time_embedding(timesteps)\n",
    "        if self.time_embedding_type != 'linear':\n",
    "            time_embeddings = time_embeddings.squeeze(-2)\n",
    "        time_embeddings = rearrange(\n",
    "            time_embeddings, '(batch block) n_embd -> batch block n_embd', block=block_size)\n",
    "        return time_embeddings\n",
    "\n",
    "    def get_state_embedding(self, states):\n",
    "        # embed states and recast back to (batch, block_size, n_embd)\n",
    "        block_size = states.shape[1]\n",
    "        if self.state_embedding_type == \"CNN\":\n",
    "            states = rearrange(\n",
    "                states, 'batch block height width channel -> (batch block) channel height width')\n",
    "            state_embeddings = self.state_encoder(states.type(\n",
    "                torch.float32).contiguous())  # (batch * block_size, n_embd)\n",
    "        else:\n",
    "            states = rearrange(\n",
    "                states, 'batch block height width channel -> (batch block) (channel height width)')\n",
    "            state_embeddings = self.state_encoder(states.type(\n",
    "                torch.float32).contiguous())  # (batch * block_size, n_embd)\n",
    "        state_embeddings = rearrange(\n",
    "            state_embeddings, '(batch block) n_embd -> batch block n_embd', block=block_size)\n",
    "        return state_embeddings\n",
    "\n",
    "    def get_reward_embedding(self, rtgs):\n",
    "        block_size = rtgs.shape[1]\n",
    "        rtgs = rearrange(rtgs, 'batch block rtg -> (batch block) rtg')\n",
    "        rtg_embeddings = self.reward_embedding(rtgs.type(torch.float32))\n",
    "        rtg_embeddings = rearrange(\n",
    "            rtg_embeddings, '(batch block) n_embd -> batch block n_embd', block=block_size)\n",
    "        return rtg_embeddings\n",
    "\n",
    "    def get_action_embedding(self, actions):\n",
    "        block_size = actions.shape[1]\n",
    "        actions = rearrange(\n",
    "            actions, 'batch block action -> (batch block) action')\n",
    "        # I don't see why we need this but we do? Maybe because of the sequential?\n",
    "        action_embeddings = self.action_embedding(actions).flatten(1)\n",
    "        action_embeddings = rearrange(\n",
    "            action_embeddings, '(batch block) n_embd -> batch block n_embd', block=block_size)\n",
    "        return action_embeddings\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_token_embeddings(self, states, actions, rtgs, timesteps):\n",
    "        '''\n",
    "        Returns the token embeddings for the transformer input.\n",
    "        Note that different subclasses will have different token embeddings\n",
    "        such as the DecisionTransformer which will use RTG (placed before the\n",
    "        state embedding).\n",
    "        \n",
    "        Args:\n",
    "            states: (batch, position, state_dim)\n",
    "            actions: (batch, position)\n",
    "            rtgs: (batch, position)\n",
    "            timesteps: (batch, position)\n",
    "\n",
    "        Returns:\n",
    "            token_embeddings: (batch, position, n_embd)\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def get_action(self, states, actions, rewards, timesteps):\n",
    "\n",
    "        state_preds, action_preds, reward_preds = self.forward(\n",
    "            states, actions, rewards, timesteps)\n",
    "\n",
    "        # get the action prediction\n",
    "        action_preds = action_preds[:, -1, :]  # (batch, n_actions)\n",
    "        action = torch.argmax(action_preds, dim=-1)  # (batch)\n",
    "        return action\n",
    "\n",
    "    def initialize_time_embedding(self):\n",
    "\n",
    "        if not self.transformer_config.linear_time_embedding:\n",
    "            self.time_embedding = nn.Embedding(\n",
    "                self.environment_config.max_steps+1, self.transformer_config.d_model)\n",
    "        else:\n",
    "            self.time_embedding = nn.Linear(\n",
    "                1, self.transformer_config.d_model)\n",
    "            \n",
    "        return self.time_embedding\n",
    "    \n",
    "    def initialize_state_embedding(self):\n",
    "\n",
    "        if self.transformer_config.state_embedding_type == 'CNN':\n",
    "            state_embedding = StateEncoder(self.transformer_config.d_model)\n",
    "        else:\n",
    "            n_obs = np.prod(self.environment_config.observation_space['image'].shape)\n",
    "            state_embedding = nn.Linear(n_obs, self.transformer_config.d_model, bias=False)\n",
    "            nn.init.normal_(state_embedding.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        return state_embedding\n",
    "    \n",
    "    def initialize_state_predictor(self):\n",
    "        if isinstance(env.observation_space, Box):\n",
    "            self.predict_states = nn.Linear(\n",
    "                self.transformer_config.d_model, np.prod(self.environment_config.observation_space.shape))\n",
    "        elif isinstance(env.observation_space, Dict):\n",
    "            self.predict_states = nn.Linear(\n",
    "                self.transformer_config.d_model, np.prod(self.environment_config.observation_space['image'].shape))\n",
    "\n",
    "    def initialize_easy_transformer(self):\n",
    "\n",
    "        # Transformer\n",
    "        cfg = HookedTransformerConfig(\n",
    "            n_layers=self.transformer_config.n_layers,\n",
    "            d_model=self.transformer_config.d_model,\n",
    "            d_head=self.transformer_config.d_head,\n",
    "            n_heads=self.transformer_config.n_heads,\n",
    "            d_mlp=self.transformer_config.d_mlp,\n",
    "            d_vocab=self.transformer_config.d_model,\n",
    "            # 3x the max timestep so we have room for an action, reward, and state per timestep\n",
    "            n_ctx=self.transformer_config.n_ctx,\n",
    "            act_fn=\"relu\",\n",
    "            normalization_type= \"LN\" if self.transformer_config.layer_norm else None,\n",
    "            attention_dir=\"causal\",\n",
    "            d_vocab_out=self.transformer_config.d_model,\n",
    "            seed=self.transformer_config.seed,\n",
    "            device=self.transformer_config.device,\n",
    "        )\n",
    "\n",
    "        assert cfg.attention_dir == \"causal\", \"Attention direction must be causal\"\n",
    "        # assert cfg.normalization_type is None, \"Normalization type must be None\"\n",
    "\n",
    "        transformer = HookedTransformer(cfg)\n",
    "\n",
    "        # Because we passing in tokens, turn off embedding and update the position embedding\n",
    "        transformer.embed = nn.Identity()\n",
    "        transformer.pos_embed = PosEmbedTokens(cfg)\n",
    "        # initialize position embedding\n",
    "        nn.init.normal_(transformer.pos_embed.W_pos,\n",
    "                        cfg.initializer_range)\n",
    "        # don't unembed, we'll do that ourselves.\n",
    "        transformer.unembed = nn.Identity()\n",
    "        \n",
    "        return transformer\n",
    "\n",
    "class DecisionTransformer(TrajectoryTransformer):\n",
    "\n",
    "    def __init__(self, environment_config, transformer_config, **kwargs):\n",
    "        super().__init__(\n",
    "            environment_config = environment_config,\n",
    "            transformer_config = transformer_config, \n",
    "            **kwargs)\n",
    "    \n",
    "    def get_token_embeddings(self,\n",
    "                             state_embeddings,\n",
    "                             time_embeddings,\n",
    "                             action_embeddings,\n",
    "                             reward_embeddings,\n",
    "                             targets=None):\n",
    "        '''\n",
    "        We need to compose the embeddings for:\n",
    "            - states\n",
    "            - actions\n",
    "            - rewards\n",
    "            - time\n",
    "\n",
    "        Handling the cases where:\n",
    "        1. we are training:\n",
    "            1. we may not have action yet (reward, state)\n",
    "            2. we have (action, state, reward)...\n",
    "        2. we are evaluating:\n",
    "            1. we have a target \"a reward\" followed by state\n",
    "\n",
    "        1.1 and 2.1 are the same, but we need to handle the target as the initial reward.\n",
    "\n",
    "        '''\n",
    "        batches = state_embeddings.shape[0]\n",
    "\n",
    "        reward_embeddings = reward_embeddings + time_embeddings\n",
    "        state_embeddings = state_embeddings + time_embeddings\n",
    "\n",
    "        if action_embeddings is not None:\n",
    "            action_embeddings = action_embeddings + time_embeddings\n",
    "        if targets:\n",
    "            targets = targets + time_embeddings\n",
    "\n",
    "        timesteps = time_embeddings.shape[1]  # number of timesteps\n",
    "        if action_embeddings is not None:\n",
    "            trajectory_length = timesteps*3\n",
    "        else:\n",
    "            trajectory_length = 2  # one timestep, no action yet\n",
    "\n",
    "        # create the token embeddings\n",
    "        token_embeddings = torch.zeros(\n",
    "            (batches, trajectory_length, self.transformer_config.d_model),\n",
    "            dtype=torch.float32, device=state_embeddings.device)  # batches, blocksize, n_embd\n",
    "\n",
    "        if action_embeddings is not None:\n",
    "            token_embeddings[:, ::3, :] = reward_embeddings\n",
    "            token_embeddings[:, 1::3, :] = state_embeddings\n",
    "            token_embeddings[:, 2::3, :] = action_embeddings\n",
    "        else:\n",
    "            token_embeddings[:, 0, :] = reward_embeddings[:, 0, :]\n",
    "            token_embeddings[:, 1, :] = state_embeddings[:, 0, :]\n",
    "\n",
    "        if targets is not None:\n",
    "            token_embeddings[:, 0, :] = targets[:, 0, :]\n",
    "\n",
    "        return token_embeddings\n",
    "\n",
    "\n",
    "transformer_config = TransformerModelConfig()\n",
    "environment_config = EnvironmentConfig()\n",
    "decision_transformer_new = DecisionTransformer(\n",
    "    transformer_config = transformer_config,\n",
    "    environment_config = environment_config,\n",
    ")\n",
    "\n",
    "env = gym.make(environment_config.env_id)\n",
    "\n",
    "decision_transformer_old = DecisionTransformerOld(\n",
    "    env = env,\n",
    "    d_model =transformer_config.d_model,\n",
    "    n_heads = transformer_config.n_heads,\n",
    "    d_mlp = transformer_config.d_mlp,\n",
    "    n_layers = transformer_config.n_layers,\n",
    "    layer_norm = transformer_config.layer_norm,\n",
    "    state_embedding_type = transformer_config.state_embedding_type,\n",
    "    time_embedding_type = \"learned\",\n",
    "    max_timestep = environment_config.max_steps,\n",
    "    n_ctx= transformer_config.n_ctx,\n",
    "    seed= transformer_config.seed,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def compare_models(model1, model2):\n",
    "    # Get the parameter dictionaries for both models\n",
    "    params1 = model1.state_dict()\n",
    "    params2 = model2.state_dict()\n",
    "\n",
    "    # Create a list of dictionaries containing information about each parameter\n",
    "    param_info = []\n",
    "    for param_name in params1.keys():\n",
    "        # Check if the parameter exists in both models\n",
    "        if param_name in params2:\n",
    "            # Get the shape of the parameter\n",
    "            shape1 = params1[param_name].shape\n",
    "            shape2 = params2[param_name].shape\n",
    "\n",
    "            # Add the parameter info to the list\n",
    "            param_info.append({'Parameter': param_name,\n",
    "                               'Model 1 Shape': shape1,\n",
    "                               'Model 2 Shape': shape2,\n",
    "                               'Sizes Match': shape1 == shape2})\n",
    "        else:\n",
    "            # Add a placeholder entry for the missing parameter\n",
    "            param_info.append({'Parameter': param_name,\n",
    "                               'Model 1 Shape': shape1,\n",
    "                               'Model 2 Shape': 'MISSING',\n",
    "                               'Sizes Match': False})\n",
    "\n",
    "    # Add entries for any parameters that exist in model2 but not model1\n",
    "    for param_name in params2.keys():\n",
    "        if param_name not in params1:\n",
    "            param_info.append({'Parameter': param_name,\n",
    "                               'Model 1 Shape': 'MISSING',\n",
    "                               'Model 2 Shape': params2[param_name].shape,\n",
    "                               'Sizes Match': False})\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(param_info)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    return df\n",
    "\n",
    "compare_models(decision_transformer_new, decision_transformer_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432411"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the total number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(decision_transformer_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432411"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(decision_transformer_old)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- prove the refactor hasn't introduced any bugs:\n",
    "    - check par count changes\n",
    "    - unit test components\n",
    "    - check that the model can learn something\n",
    "    - check that each hyperparameter works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision_transformer_interpretability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0be95f1efa079bcf597630754731f3dc2b2137553763cb34bfd5652600bd2735"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
